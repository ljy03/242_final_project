{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0f6307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] END ......................clf__C=0.1, clf__solver=lbfgs; total time=   4.2s\n",
      "[CV] END ......................clf__C=0.1, clf__solver=lbfgs; total time=   4.6s\n",
      "[CV] END ......................clf__C=0.1, clf__solver=lbfgs; total time=   5.2s\n",
      "[CV] END ........................clf__C=1, clf__solver=lbfgs; total time=   5.5s\n",
      "[CV] END ........................clf__C=1, clf__solver=lbfgs; total time=   5.7s\n",
      "[CV] END ........................clf__C=1, clf__solver=lbfgs; total time=   4.3s\n",
      "[CV] END .......................clf__C=10, clf__solver=lbfgs; total time=   5.4s\n",
      "[CV] END .......................clf__C=10, clf__solver=lbfgs; total time=   5.0s\n",
      "[CV] END .......................clf__C=10, clf__solver=lbfgs; total time=   4.9s\n",
      "[CV] END ..................clf__C=0.1, clf__solver=liblinear; total time=  17.7s\n",
      "[CV] END ..................clf__C=0.1, clf__solver=liblinear; total time=  19.5s\n",
      "[CV] END ..................clf__C=0.1, clf__solver=liblinear; total time=  19.7s\n",
      "[CV] END ....................clf__C=1, clf__solver=liblinear; total time=  17.9s\n",
      "[CV] END ....................clf__C=1, clf__solver=liblinear; total time=  20.4s\n",
      "[CV] END ....................clf__C=1, clf__solver=liblinear; total time=  20.6s\n",
      "[CV] END ...................clf__C=10, clf__solver=liblinear; total time=  15.0s\n",
      "[CV] END ...................clf__C=10, clf__solver=liblinear; total time=  12.5s\n",
      "[CV] END ...................clf__C=10, clf__solver=liblinear; total time=  10.9s\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] END .........................................clf__C=0.1; total time=  16.5s\n",
      "[CV] END ...........................................clf__C=1; total time=  17.2s\n",
      "[CV] END .........................................clf__C=0.1; total time=  17.4s\n",
      "[CV] END ...........................................clf__C=1; total time=  19.5s\n",
      "[CV] END ..........................................clf__C=10; total time=  19.5s\n",
      "[CV] END .........................................clf__C=0.1; total time=  20.4s\n",
      "[CV] END ..........................................clf__C=10; total time=  20.3s\n",
      "[CV] END ...........................................clf__C=1; total time=  20.5s\n",
      "[CV] END ..........................................clf__C=10; total time=   6.8s\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=2, n_estimators=200; total time= 2.9min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=2, n_estimators=200; total time= 3.0min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=2, n_estimators=400; total time= 5.6min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=2, n_estimators=400; total time= 5.6min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=2, n_estimators=400; total time= 5.6min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=2, n_estimators=400; total time= 5.9min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=2, n_estimators=200; total time= 3.0min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=2, n_estimators=400; total time= 6.0min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=2, n_estimators=400; total time= 6.0min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=5, n_estimators=400; total time= 5.7min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=5, n_estimators=200; total time= 3.0min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=2, n_estimators=200; total time= 2.8min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=2, n_estimators=200; total time= 2.8min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=5, n_estimators=200; total time= 2.9min\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_split=5, n_estimators=200; total time= 2.9min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=5, n_estimators=400; total time= 4.1min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=5, n_estimators=400; total time= 4.1min\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_split=2, n_estimators=200; total time= 1.1min\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, subsample=0.8; total time=  33.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, subsample=0.8; total time=  34.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, subsample=0.8; total time=  55.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, subsample=0.8; total time=  55.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, subsample=0.8; total time=  56.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, subsample=0.8; total time=  30.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, subsample=0.8; total time=  30.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, subsample=0.8; total time=  31.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, subsample=0.8; total time=  31.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, subsample=1.0; total time= 1.0min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, subsample=1.0; total time= 1.1min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=7, subsample=0.8; total time= 2.5min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=7, subsample=0.8; total time= 2.5min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=7, subsample=0.8; total time= 2.5min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, subsample=1.0; total time=  37.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, subsample=1.0; total time= 2.2min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, subsample=1.0; total time=  36.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, subsample=1.0; total time=  37.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, subsample=1.0; total time=  35.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, subsample=1.0; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, subsample=1.0; total time= 2.2min\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, subsample=1.0; total time=  50.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, subsample=1.0; total time=  16.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, subsample=1.0; total time=  15.9s\n",
      "Saved: embedding_model_comparison.csv\n",
      "Best embedding model: LogReg, F1=0.8277\n",
      "          Model                                         BestParams  Accuracy  \\\n",
      "0        LogReg        {'clf__solver': 'liblinear', 'clf__C': 0.1}    0.8273   \n",
      "1     LinearSVM                                     {'clf__C': 10}    0.8259   \n",
      "3       XGBoost  {'subsample': 1.0, 'max_depth': 7, 'learning_r...    0.8188   \n",
      "2  RandomForest  {'n_estimators': 400, 'min_samples_split': 5, ...    0.7860   \n",
      "\n",
      "   Precision  Recall        F1  \n",
      "0   0.825931  0.8294  0.827662  \n",
      "1   0.823635  0.8294  0.826507  \n",
      "3   0.810358  0.8324  0.821231  \n",
      "2   0.770526  0.8146  0.791950  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Load embeddings + labels\n",
    "# =========================\n",
    "X = np.load(\"bert_embeddings.npy\")  # e.g., (50000, 384)\n",
    "\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")  # must align with embeddings row order\n",
    "y = (df[\"sentiment\"].str.lower() == \"positive\").astype(int).to_numpy()\n",
    "\n",
    "assert len(y) == X.shape[0], f\"y length {len(y)} != X rows {X.shape[0]}\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) Split data\n",
    "# =========================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# For final XGBoost early stopping refit (NOT used in CV)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "def eval_binary(y_true, y_pred):\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Define models + SMALL randomized searches\n",
    "# =========================\n",
    "models = {\n",
    "    \"LogReg\": {\n",
    "        \"estimator\": Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", LogisticRegression(max_iter=3000))\n",
    "        ]),\n",
    "        \"param_distributions\": {\n",
    "            \"clf__C\": [0.1, 1, 10],\n",
    "            \"clf__solver\": [\"lbfgs\", \"liblinear\"],\n",
    "        },\n",
    "        \"n_iter\": 6,\n",
    "    },\n",
    "\n",
    "    \"LinearSVM\": {\n",
    "        \"estimator\": Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"clf\", LinearSVC())\n",
    "        ]),\n",
    "        \"param_distributions\": {\n",
    "            \"clf__C\": [0.1, 1, 10],\n",
    "        },\n",
    "        \"n_iter\": 3,\n",
    "    },\n",
    "\n",
    "    \"RandomForest\": {\n",
    "        \"estimator\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        \"param_distributions\": {\n",
    "            \"n_estimators\": [200, 400],\n",
    "            \"max_depth\": [None, 20],\n",
    "            \"max_features\": [\"sqrt\"],\n",
    "            \"min_samples_split\": [2, 5],\n",
    "        },\n",
    "        \"n_iter\": 6,\n",
    "    },\n",
    "\n",
    "    # XGBoost: search WITHOUT early stopping first (fast + clean with CV)\n",
    "    \"XGBoost\": {\n",
    "        \"estimator\": XGBClassifier(\n",
    "            objective=\"binary:logistic\",\n",
    "            eval_metric=\"logloss\",     \n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            tree_method=\"hist\",\n",
    "            n_estimators=300           # keep small for tuning speed\n",
    "        ),\n",
    "        \"param_distributions\": {\n",
    "            \"max_depth\": [3, 5, 7],\n",
    "            \"learning_rate\": [0.05, 0.1],\n",
    "            \"subsample\": [0.8, 1.0],\n",
    "            \"colsample_bytree\": [0.8, 1.0],\n",
    "        },\n",
    "        \"n_iter\": 8,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) Train, tune, evaluate\n",
    "# =========================\n",
    "rows = []\n",
    "best_name, best_model, best_f1 = None, None, -1\n",
    "\n",
    "best_xgb_params = None  # store best params for later early-stopping refit\n",
    "\n",
    "for name, cfg in models.items():\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=cfg[\"estimator\"],\n",
    "        param_distributions=cfg[\"param_distributions\"],\n",
    "        n_iter=cfg[\"n_iter\"],\n",
    "        scoring=\"f1\",\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "        error_score=\"raise\",\n",
    "        refit=True\n",
    "    )\n",
    "\n",
    "    # CV fit (fast)\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on test\n",
    "    y_pred = search.best_estimator_.predict(X_test)\n",
    "    metrics = eval_binary(y_test, y_pred)\n",
    "\n",
    "    rows.append({\n",
    "        \"Model\": name,\n",
    "        \"BestParams\": search.best_params_,\n",
    "        **metrics\n",
    "    })\n",
    "\n",
    "    # Track best overall model (temporary for XGB; we will refit with early stopping below)\n",
    "    if metrics[\"F1\"] > best_f1:\n",
    "        best_f1 = metrics[\"F1\"]\n",
    "        best_name = name\n",
    "        best_model = search.best_estimator_\n",
    "\n",
    "    if name == \"XGBoost\":\n",
    "        best_xgb_params = search.best_params_\n",
    "\n",
    "\n",
    "if best_xgb_params is not None:\n",
    "    xgb_final = XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",\n",
    "        n_estimators=2000,           \n",
    "        early_stopping_rounds=30,     \n",
    "        **best_xgb_params\n",
    "    )\n",
    "\n",
    "    xgb_final.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    y_pred_xgb = xgb_final.predict(X_test)\n",
    "    metrics_xgb = eval_binary(y_test, y_pred_xgb)\n",
    "\n",
    "    # Update the XGBoost row to reflect final early-stopping performance\n",
    "    for r in rows:\n",
    "        if r[\"Model\"] == \"XGBoost\":\n",
    "            r[\"Accuracy\"] = metrics_xgb[\"Accuracy\"]\n",
    "            r[\"Precision\"] = metrics_xgb[\"Precision\"]\n",
    "            r[\"Recall\"] = metrics_xgb[\"Recall\"]\n",
    "            r[\"F1\"] = metrics_xgb[\"F1\"]\n",
    "            r[\"BestParams\"] = {**best_xgb_params, \"early_stopping_rounds\": 30, \"n_estimators(max)\": 2000}\n",
    "            break\n",
    "\n",
    "    # Update best model if XGBoost final becomes best\n",
    "    if metrics_xgb[\"F1\"] > best_f1:\n",
    "        best_f1 = metrics_xgb[\"F1\"]\n",
    "        best_name = \"XGBoost\"\n",
    "        best_model = xgb_final\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) Outputs\n",
    "# =========================\n",
    "results = pd.DataFrame(rows).sort_values(\"F1\", ascending=False)\n",
    "results.to_csv(\"embedding_model_comparison.csv\", index=False)\n",
    "joblib.dump(best_model, f\"best_embedding_model_{best_name}.joblib\")\n",
    "\n",
    "print(\"Saved: embedding_model_comparison.csv\")\n",
    "print(f\"Best embedding model: {best_name}, F1={best_f1:.4f}\")\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
